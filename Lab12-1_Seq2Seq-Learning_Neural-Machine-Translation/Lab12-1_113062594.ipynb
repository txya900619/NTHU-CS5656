{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the first GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "movie_reviews = pd.read_csv(\"./data/IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if there is any null value in the dataset\n",
    "movie_reviews.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training data: 40000\n",
      "# test data: 10000\n"
     ]
    }
   ],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence\n",
    "X = []\n",
    "sentences = list(movie_reviews['review'])\n",
    "for sen in sentences:\n",
    "    X.append(preprocess_text(sen))\n",
    "\n",
    "# replace the positive with 1, replace the negative with 0\n",
    "y = movie_reviews['sentiment']\n",
    "y = np.array(list(map(lambda x: 1 if x == \"positive\" else 0, y)))\n",
    "# Split the training dataset and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "print(\"# training data: {:d}\\n# test data: {:d}\".format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_len = 100\n",
    "# padding sentences to the same length\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post', maxlen=max_len)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([128, 100]), TensorShape([128]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(X_train)\n",
    "BATCH_SIZE = 128\n",
    "steps_per_epoch = len(X_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "# only reserve 10000 words\n",
    "vocab_size = 10000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=False)\n",
    "\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (128, 100, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (128, 1024)\n",
      "tf.Tensor([ True  True  True ...  True  True  True], shape=(1024,), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        # vacab_size=10000, embedding_dim=256 enc_units=1024 batch_sz=64\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_activation='sigmoid',\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        # x is the training data with shape == (batch_sizeï¼Œmax_length)  -> (128, 100)\n",
    "        # which means there are batch_size sentences in one batch, the length of each sentence is max_length\n",
    "        # hidden state shape == (batch_size, units) -> (128, 1024)\n",
    "        # after embedding, x shape == (batch_size, max_length, embedding_dim) -> (128, 100, 256)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # output contains the state(in GRU, the hidden state and the output are same) from all timestamps,\n",
    "        # output shape == (batch_size, max_length, units) -> (128, 100, 1024)\n",
    "        # state is the hidden state of the last timestamp, shape == (batch_size, units) -> (128, 1024)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        \n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        # initialize the first state of the gru,  shape == (batch_size, units) -> (128, 1024)\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "encoder = Encoder(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n",
    "# the output and the hidden state of GRU is equal\n",
    "print(sample_output[-1, -1, :] == sample_hidden[-1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.w = tf.keras.layers.Dense(units)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # TODO: Implement the Luong attention.\n",
    "        query = tf.expand_dims(query, 1)\n",
    "\n",
    "        score = tf.matmul(query, self.w(values), transpose_b=True)\n",
    "        score = tf.transpose(score, [0, 2, 1])\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (128, 1)\n"
     ]
    }
   ],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        \n",
    "        # pass through four fully connected layers, the model will return \n",
    "        # the probability of the positivity of the sentence\n",
    "        self.fc_1 = tf.keras.layers.Dense(2048)\n",
    "        self.fc_2 = tf.keras.layers.Dense(512)\n",
    "        self.fc_3 = tf.keras.layers.Dense(64)\n",
    "        self.fc_4 = tf.keras.layers.Dense(1)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = LuongAttention(self.dec_units)\n",
    "\n",
    "    def call(self, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        output = self.fc_1(context_vector)\n",
    "        output = self.fc_2(output)\n",
    "        output = self.fc_3(output)\n",
    "        output = self.fc_4(output)\n",
    "\n",
    "        return output, attention_weights\n",
    "decoder = Decoder(units, BATCH_SIZE)\n",
    "sample_decoder_output, _ = decoder(sample_hidden, sample_output)\n",
    "print('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.1828\n",
      "Epoch 1 Batch 100 Loss 0.3178\n",
      "Epoch 1 Batch 200 Loss 0.2499\n",
      "Epoch 1 Batch 300 Loss 0.2425\n",
      "Epoch 1 Loss 0.2253\n",
      "Time taken for 1 epoch 32.1752655506134 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.1497\n",
      "Epoch 2 Batch 100 Loss 0.1251\n",
      "Epoch 2 Batch 200 Loss 0.1866\n",
      "Epoch 2 Batch 300 Loss 0.1142\n",
      "Epoch 2 Loss 0.1569\n",
      "Time taken for 1 epoch 13.24014925956726 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.1127\n",
      "Epoch 3 Batch 100 Loss 0.1412\n",
      "Epoch 3 Batch 200 Loss 0.0551\n",
      "Epoch 3 Batch 300 Loss 0.1063\n",
      "Epoch 3 Loss 0.0998\n",
      "Time taken for 1 epoch 10.405840158462524 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.0341\n",
      "Epoch 4 Batch 100 Loss 0.0703\n",
      "Epoch 4 Batch 200 Loss 0.1091\n",
      "Epoch 4 Batch 300 Loss 0.0296\n",
      "Epoch 4 Loss 0.0624\n",
      "Time taken for 1 epoch 9.215907573699951 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.0191\n",
      "Epoch 5 Batch 100 Loss 0.0532\n",
      "Epoch 5 Batch 200 Loss 0.0786\n",
      "Epoch 5 Batch 300 Loss 0.0521\n",
      "Epoch 5 Loss 0.0439\n",
      "Time taken for 1 epoch 8.67432975769043 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.0045\n",
      "Epoch 6 Batch 100 Loss 0.0503\n",
      "Epoch 6 Batch 200 Loss 0.0557\n",
      "Epoch 6 Batch 300 Loss 0.0229\n",
      "Epoch 6 Loss 0.0419\n",
      "Time taken for 1 epoch 8.522937297821045 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0170\n",
      "Epoch 7 Batch 100 Loss 0.1417\n",
      "Epoch 7 Batch 200 Loss 0.0234\n",
      "Epoch 7 Batch 300 Loss 0.0175\n",
      "Epoch 7 Loss 0.0254\n",
      "Time taken for 1 epoch 8.474684953689575 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0058\n",
      "Epoch 8 Batch 100 Loss 0.0142\n",
      "Epoch 8 Batch 200 Loss 0.0299\n",
      "Epoch 8 Batch 300 Loss 0.0046\n",
      "Epoch 8 Loss 0.0238\n",
      "Time taken for 1 epoch 8.62391996383667 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0262\n",
      "Epoch 9 Batch 100 Loss 0.0056\n",
      "Epoch 9 Batch 200 Loss 0.0246\n",
      "Epoch 9 Batch 300 Loss 0.1049\n",
      "Epoch 9 Loss 0.0239\n",
      "Time taken for 1 epoch 8.084984302520752 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0592\n",
      "Epoch 10 Batch 100 Loss 0.0202\n",
      "Epoch 10 Batch 200 Loss 0.0107\n",
      "Epoch 10 Batch 300 Loss 0.0150\n",
      "Epoch 10 Loss 0.0351\n",
      "Time taken for 1 epoch 8.395703554153442 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    loss_ = loss_object(real, pred)\n",
    "    return tf.reduce_mean(loss_)\n",
    "checkpoint_dir = './checkpoints/sentiment-analysis'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        # passing enc_output to the decoder\n",
    "        predictions, _ = decoder(enc_hidden, enc_output)\n",
    "\n",
    "        loss = loss_function(targ, predictions)\n",
    "\n",
    "    # collect all trainable variables\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    # calculate the gradients for the whole variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    # apply the gradients on the variables\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss\n",
    "# set the epochs for training\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    # get the initial hidden state of gru\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoints/sentiment-analysis/ckpt-5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7fdae3df0df0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(inp, enc_hidden):\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        predictions, attention_weights = decoder(enc_hidden, enc_output)\n",
    "    return predictions, attention_weights\n",
    "def evaluate(test_data):\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    \n",
    "    for batch, (inp, targ) in enumerate(test_data):\n",
    "        if len(inp) != BATCH_SIZE:\n",
    "            enc_hidden = tf.zeros((len(inp), units))\n",
    "        # make prediction\n",
    "        if batch == 0:\n",
    "            predictions, attention_weights = test_step(inp, enc_hidden)\n",
    "            predictions, attention_weights = predictions.numpy(), attention_weights.numpy()\n",
    "        else:\n",
    "            _predictions, _attention_weights = test_step(inp, enc_hidden)\n",
    "            _predictions, _attention_weights = _predictions.numpy(), _attention_weights.numpy()\n",
    "            predictions = np.concatenate((predictions, _predictions))\n",
    "            attention_weights = np.concatenate((attention_weights, _attention_weights))\n",
    "    \n",
    "    predictions = np.squeeze(predictions)\n",
    "    attention_weights = np.squeeze(attention_weights)\n",
    "    predictions[np.where(predictions < 0.5)] = 0\n",
    "    predictions[np.where(predictions >= 0.5)] = 1\n",
    "    return predictions, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8428\n"
     ]
    }
   ],
   "source": [
    "y_pred, attention_weights = evaluate(test_dataset)\n",
    "print('Accuracy: ', (y_pred == y_test).sum() / len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true: 1\n",
      "y_predict: 0\n",
      "changed it was \u001b[31mterrible\u001b[0m main event just like every match is in is terrible other matches \u001b[31mon\u001b[0m the card were razor \u001b[31mramon\u001b[0m \u001b[31mvs\u001b[0m ted brothers vs bodies shawn michaels vs this was the event where shawn named his big monster of body guard vs kid hart first takes on then takes on jerry and stuff with the and was always very interesting then destroyed marty undertaker took on giant in another terrible \u001b[31mmatch\u001b[0m \u001b[31mthe\u001b[0m \u001b[31msmoking\u001b[0m and took on bam bam and the and the world title against lex this match was \u001b[31mboring\u001b[0m \u001b[31mand\u001b[0m \u001b[31mit\u001b[0m has terrible ending however it deserves \n",
      "\n",
      "\n",
      "y_true: 1\n",
      "y_predict: 1\n",
      "of subject matter as are and broken in \u001b[31mmany\u001b[0m \u001b[31mways\u001b[0m on \u001b[31mmany\u001b[0m \u001b[31mmany\u001b[0m issues \u001b[31mhappened\u001b[0m \u001b[31mto\u001b[0m see the pilot premiere in passing and just had to keep in after that to see if would ever get the girl after seeing them all on television was delighted to see them available \u001b[31mon\u001b[0m dvd have to admit that it was the only thing that kept me sane whilst had to do hour night \u001b[31mshift\u001b[0m and developed insomnia farscape \u001b[31mwas\u001b[0m the only thing to get me through those extremely long nights do \u001b[31myourself\u001b[0m favour watch the pilot and see what mean farscape comet \n",
      "\n",
      "\n",
      "y_true: 0\n",
      "y_predict: 0\n",
      "destruction the first really bad thing is the guy steven seagal would have been beaten to \u001b[31mpulp\u001b[0m by seagal driving but that probably would have ended the whole premise for the movie it seems like they decided to make all kinds of changes in the movie \u001b[31mplot\u001b[0m so just \u001b[31mplan\u001b[0m \u001b[31mto\u001b[0m \u001b[31menjoy\u001b[0m \u001b[31mthe\u001b[0m \u001b[31maction\u001b[0m \u001b[31mand\u001b[0m \u001b[31mdo\u001b[0m not expect coherent plot turn any sense of logic you may have it will your chance of getting headache does give me some hope that steven seagal is \u001b[31mtrying\u001b[0m to move back towards the type of characters he portrayed in his more popular movies \n",
      "\n",
      "\n",
      "y_true: 1\n",
      "y_predict: 1\n",
      "\u001b[31mjane\u001b[0m austen would definitely of this one paltrow does an awesome job \u001b[31mcapturing\u001b[0m the \u001b[31mattitude\u001b[0m of emma she is funny without being silly yet elegant she puts on \u001b[31mvery\u001b[0m convincing british accent not being british myself maybe m not the best judge but she fooled me she was also \u001b[31mexcellent\u001b[0m \u001b[31min\u001b[0m doors sometimes forget she american also brilliant \u001b[31mare\u001b[0m \u001b[31mjeremy\u001b[0m northam and \u001b[31msophie\u001b[0m thompson and law emma thompson sister and mother as the bates women they nearly steal the show and ms law doesn even have any lines highly \u001b[31mrecommended\u001b[0m \n",
      "\n",
      "\n",
      "y_true: 0\n",
      "y_predict: 0\n",
      "reaches the point where they become obnoxious and simply \u001b[31mfrustrating\u001b[0m touch football puzzle family and talent shows are not how actual people behave it almost \u001b[31msickening\u001b[0m another big flaw is the woman carell is \u001b[31msupposed\u001b[0m to \u001b[31mbe\u001b[0m falling \u001b[31mfor\u001b[0m her in her \u001b[31mfirst\u001b[0m \u001b[31mscene\u001b[0m with steve carell is like watching stroke victim trying to be what imagine is \u001b[31msupposed\u001b[0m to \u001b[31mbe\u001b[0m unique and original in this \u001b[31mwoman\u001b[0m comes off as mildly retarded it makes me think that this movie is taking place on another planet left the theater wondering what just saw after thinking further don think it was much \n",
      "\n",
      "\n",
      "y_true: 1\n",
      "y_predict: 1\n",
      "the \u001b[31mpace\u001b[0m quick and energetic but \u001b[31mmost\u001b[0m importantly he knows how to make comedy funny he doesn the jokes and he \u001b[31munderstands\u001b[0m \u001b[31mthat\u001b[0m funny actors know what they re doing and he allows them to do it but segal goes step further he \u001b[31mgives\u001b[0m \u001b[31mtommy\u001b[0m boy \u001b[31mfriendly\u001b[0m almost nostalgic tone that both the genuinely and the critics didn like tommy boy shame on them movie doesn have to be super sophisticated or intellectual to be funny god \u001b[31mfarley\u001b[0m and \u001b[31mspade\u001b[0m were forced to do muted comedy la the office this is great \u001b[31mmovie\u001b[0m and one of my all time favorites \n",
      "\n",
      "\n",
      "y_true: 1\n",
      "y_predict: 1\n",
      "for once story of hope over the tragic reality our \u001b[31myouth\u001b[0m face rising draws one into scary and unfair world and shows through beautiful color and \u001b[31mmoving\u001b[0m music how one man and his dedicated friends choose not to accept that world and change it through action and art an \u001b[31mentertaining\u001b[0m interesting emotional beautiful film showed this \u001b[31mfilm\u001b[0m to numerous high \u001b[31mschool\u001b[0m students \u001b[31mas\u001b[0m well who all \u001b[31mlive\u001b[0m in with poverty and and gun violence and they were with anderson the protagonist recommend this film to all \u001b[31mages\u001b[0m over due to subtitles and some images of \u001b[31mdeath\u001b[0m from all \u001b[31mbackgrounds\u001b[0m \n",
      "\n",
      "\n",
      "y_true: 1\n",
      "y_predict: 1\n",
      "people and sleeping around that he kept secret from most people he feels free to have an \u001b[31maffair\u001b[0m with quasi because he kevin he figures \u001b[31mout\u001b[0m that he can fool some people with cards like hotel but it won get him out of those the of heaven are keeping track of him and everything he does \u001b[31mafter\u001b[0m reading all the theories on \u001b[31mthough\u001b[0m it seems like identity is reminder \u001b[31mof\u001b[0m \u001b[31mthe\u001b[0m different paths \u001b[31mtony\u001b[0m \u001b[31mcould\u001b[0m \u001b[31mve\u001b[0m \u001b[31mtaken\u001b[0m in his life possibly along with the car joke involving that made no sense to me otherwise at that point my brain out \n",
      "\n",
      "\n",
      "y_true: 0\n",
      "y_predict: 0\n",
      "over again can remember how \u001b[31mmany\u001b[0m times he said the universe is made out of \u001b[31mtiny\u001b[0m little strings it like they were trying to us into just \u001b[31maccepting\u001b[0m \u001b[31mare\u001b[0m the best thing since bread \u001b[31mfinally\u001b[0m the show ended off with an unpleasant sense of \u001b[31mcompetition\u001b[0m between and clearly biased towards this \u001b[31mis\u001b[0m supposed \u001b[31mto\u001b[0m be an educational program about quantum \u001b[31mphysics\u001b[0m not \u001b[31mabout\u001b[0m whether the us is better than europe or vice versa also felt that was part of the audiences need to see some conflict to remain interested please give me little more credit than that overall thumbs down \n",
      "\n",
      "\n",
      "y_true: 0\n",
      "y_predict: 1\n",
      "the scenes involving \u001b[31mjoe\u001b[0m character in particular the scenes in the terribly clich but still funny rich but screwed up characters house where the story \u001b[31mtowards\u001b[0m it final moments can \u001b[31msee\u001b[0m how was great stage \u001b[31mplay\u001b[0m and while the film makers did their \u001b[31mbest\u001b[0m \u001b[31mto\u001b[0m translate this to celluloid it simply didn work and while laughed \u001b[31mout\u001b[0m loud at some of scenes and one \u001b[31mliners\u001b[0m think the first minutes my \u001b[31msenses\u001b[0m \u001b[31mand\u001b[0m expectations to such degree would have laughed at anything unless you re stuck for novelty coffee coaster don pick this up if you see it in bargain bucket \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "for idx, data in enumerate(X_test[:10]):\n",
    "    print('y_true: {:d}'.format(y_test[idx]))\n",
    "    print('y_predict: {:.0f}'.format(y_pred[idx]))\n",
    "    \n",
    "    # get the twenty most largest attention weights\n",
    "    large_weights_idx = np.argsort(attention_weights[idx])[::-1][:10]\n",
    "    \n",
    "    for _idx in range(len(data)):\n",
    "        word_idx = data[_idx]\n",
    "        if word_idx != 0:\n",
    "            if _idx in large_weights_idx:\n",
    "                print(colored(tokenizer.index_word[word_idx], 'red'), end=' ')\n",
    "                # try this if termcolor is not working properly\n",
    "                # print(f'\\033[31m{tokenizer.index_word[word_idx]}\\033[0m', end=' ')\n",
    "            else:\n",
    "                print(tokenizer.index_word[word_idx], end=' ')\n",
    "    print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NTHU-CS5656",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
